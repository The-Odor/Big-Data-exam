% Do not modify these
\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}



% -- Insert any custom LaTeX packages here --

% \package{natbib} % <-- Required for the Chicago citation style
% \package{apacite} % <-- Required for the APA citation style
% If you decide to use one of the styles above, remember to change the \bibliographystyle{} at the bottom of the document too!

\usepackage{listings} % <-- Required if you want to display program source code in your paper.
\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
    }
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\urlstyle{same}

% -- End of custom LaTeX packages --


% Fill in your title
\title{Exploring Bitcoin }

% Do not modify the author tag below, just let it be blank
\author{}

% Fill in assignment abstract
\begin{abstract}
In this assignment we parsed a dataset, in the form of XMLs, through a Hadoop- and Pig apache. We learned how to write MapReduce jobs and Pig scripts, and how effective it is on our dataset based on XML files from Bitcoin stack exchange. 

\end{abstract}


% Do not modify the following two lines
\lstset{style=mystyle}
\begin{document}
\include{cover}


% Insert data for the hand-in's cover page
\makecoverpage{
	master_of 		 = \par{Applied Computer Science},  % Use either: Applied Computer Science | Human-Computer Interaction
	assignment_title = \par{ Exploring Bitcoin} ,  % Title of your assignment
	course_code    	 = \par{MA120},  % Course code (ex. MA110)
	course_name      = \par{Big Data},  % Course name (ex. Systems Development)
	due_date		 = \par{18.October 2019},  % Due date
	student_name     = \par{Theodor Midtbø Alstad ; Howie Chen},  % Your name (or names, if group – separate names with ; semicolon)
	student_number   = \par{865317 ; 866354},  % Your student ID number (or numbers, if group – separate ID numbers with ; semicolon)
	group_size		 = 2, % Number of group members (used for the declaration text)
}


% Do not modify the following two lines
\flushbottom
\maketitle

\tableofcontents
\newpage
% --INTRODUCTION--
\section{Introduction}
This report is of a structure where each task is separately reported, referring to eachother due to their similarity. Each contains our assumptions about what the task is asking for, implementation, and any notes/reflections if necessary, opting to add notes/reflection more than necessary rather than less than necessary.
We chose to work together because both of us have python background, choosing python as main programming language. We explored a Bitcoin StackExchange dataset based in XML-files as given from \url{archive.org/download/stackexchange/bitcoin.stackexchange.com.7z} (accessed 16.10.2019) through Hadoop and Pig. 
Hadoop is used to store and process big data, it has enormous processing power potential and has the ability to handle virtually any number of tasks and jobs. Hadoop provides a framework which allow users to write and test distributed systems and does not rely on hardware to provide fault tolerance, making it less apt for our small-scale applications, yet we use it for the experience. Pig is a MapReduce abstraction, working on hadoop methods, that we use in some tasks due to its efficiency. We have elected to use both pig-scripts and pure MapReduce for very similar tasks (see 2c and 2d) to both show our ability to employ both methods and get experience with the two methods.

%to both show our understanding and get experience with both
%
%to show experience in methods and understanding in both Apache.
%
%to show our ability to employ, and get experience with, both
%
%to both show our ability to employ both methods and get experience with both.

% --FUNCTIOS %-- 
\section{Main functions}
The datasets contains only XML files, which it means values to different attributes have a lot of  ascii characters, punctuation, numbers and HTML tags. We also needed a function to parse the XML files to Hadoop and to create a mapper for the data.  Therefor we created three different main functions thats being reused through the project. : 
\subsection{CleanBody}

In cleanBody function it formats strings for mapper function. The function makes non-case sensitivity, removes ascii characters , HTML formatting, and treats anything separated by blanked space or as separated words. Most of the words after separating/splitting will be counted as separate words, and this will effect the results of some tasks. For instance  the name "Jens-Petter" will be counted as two words; "Jens" and "Petter". The input for the function is a string body which will be formatted and it returns a list of formatted string.

\begin{lstlisting}[language=Python, caption=cleanBody function]
def cleanBody(body):
    body = body.lower()
    body = ascii(body)
    body = sub("<.+?>","",body)
    body = body.replace("/", " ")
    body = body.strip()

    for i in ignore_char:
        	body = body.replace(i, "")

    body = body.split(" ")

    return body
\end{lstlisting}
\subsection{Mapper Core}
mapper\_core is the core of the mapper function; it prints out the relevant data in a format parseable by Hadoop. It functions through three modes, as determined by the parameter mode: "single", "double", and "triple".

\begin{itemize}
  \item Single: Assumes input "words" is a list of words to print. Prints word in words as (word, 1). Ignores empty strings and spaces.
  \item Double: Assumes input "words" is two nested lists to print. Prints word, count in words as (word, count). Ignores empty strings and spaces.
  \item Triple: Assumes input "words" is three nested lists to print. Prints id, score, title in words as (id, score, title). does not ignore empty strings and spaces.
\end{itemize}

\begin{lstlisting}[language=Python, caption=mapper\_core function]
def mapper_core(words, mode="single"):
    if mode == "single":
        for word in words:
            if word not in ["", " "]:
              print("%s %s" %(word,1)) #Emit the word

    elif mode == "double":
        in1, in2 = words
        for word, count in zip(in1,in2):
            if word not in ["", " "]:
                print("%s %s" %(word,count)) #emit the words

    elif mode == "triple":
        in1, in2,in3  = words
        for id, score, title in zip(in1,in2,in3):
            print("%s %s %s" %(id,score, title)) #emit the words


\end{lstlisting}

\subsection{Xmlparser}
The chosen method to interpret the dataset is to parse to the mapper function. Although parsing an entire XML-file takes up significant memory, this method fits our dataset. It has been separated out as a function so it may be easily replaced by other methods more fit for large files. The input for this function is a XML-file and the output is a parsed XML-file 

\begin{lstlisting}[language=Python, caption=xmlparser function]
def xmlparser(infile):
    if not isinstance(infile, str):
        infile = infile.detach()
    mytree = ET.parse(infile)
    myroot = mytree.getroot()
    return myroot
\end{lstlisting}
% --TASK1--
\section{Task 1 Warmup}
%This part of the task is about get to know how hadoop- and pig apache works and how to parse XML  files throught python code. 
\subsection{WordCount}
\textbf{Assumption}: Count the words in body of questions PostTypeID="1"  in the \textit{Posts.xml}  file. The result should be how many times a word occur in the body of questions.\\ \\
\textbf{Implementation} The mapper uses the xmlparser function to parse an XML-file, iterates through all rows in parsed file, cleans text using the cleanBody function and prints output for the reducer script by using the mapper\_core function.
The reducer receives the mapper output, parses it and loops through the output. A counter variable is used to track repeated words, outputting the word with the counter when the word stops repeating. \\ \\
\textbf{Notes/Reflection} Here the choices made building the cleanBody function affects the output, interpreting some things as words that clearly are not. See line 7 in 1a\_output for example; a< br >, an html line break element, has been turned into the not-word abr. \\ \\

\lstinputlisting[firstline=500, lastline=510, title=1a\_output]{output1a.txt}

\subsection{Unique words}
\textbf{Assumption}: Write a MapReduce job where the result should be unique words in the titles PostTypeID="1" in the \textit{Posts.xml} File. \\ \\
\textbf{Implementation} This MapReduce job functions as 1a WordCount does, except it does not count the words and it looks through the titles instead of the bodies. \\ \\
\textbf{Notes/Reflection} Here again the choices made building the cleanBody function affects the output.
\lstinputlisting[firstline=500,lastline=510,title=1b\_output]{output1b.txt}

\subsection{MoreThan10}
\textbf{Assumption}: Write a simple python code to check the title length in \textit{Posts.xml}. The result should output how many titles have more than 10 words.  \\ \\
\textbf{Implementation} The mapper functions as the mapper in 1a WordCount, except it keeps a counter for the titles of more than 10 words and it outputs the final counter instead of using mapper\_core.
The reducer receives the mapper outputs and sums them up \\ \\
\textbf{Notes/Reflection} This MapReduce job is simpler, the reducer is unnecessary outside an environment with clusters.
\lstinputlisting[firstline=1,lastline=1,title=1c\_output]{output1c.txt}

\subsection{Stopwords}
\textbf{Assumption}: Write a simple python code based on task 1a to exclude \href{https://raw.githubusercontent.com/naimdjon/stopwords/master/stopwords.txt}{stopwords} from body of questions PostTypeID="1" in the \textit{Posts.xml}. The output should be text file without any stopwords. \\ \\
\textbf{Implementation} This MapReduce job functions as 1a WordCount does, except it has a section for the iterative removal of stopwords, as defined in \href{https://raw.githubusercontent.com/naimdjon/stopwords/master/stopwords.txt}{StopWords.txt} \\ \\
\textbf{Notes/Reflection} Given how we built cleanBody, we chose to remove the apostrophes (') from the stopwords, turning words like "we're" to "were"
\lstinputlisting[firstline=500,lastline=509,title=1d\_output]{output1d.txt}


\subsection{Pig top 10}
\textbf{Assumption}: Write a pig script to select top 10 listed words after removing the stopwords from \textit{Posts.xml}. The output should print out top 10 listed words and the corresponding occurrence rate.\\ \\
\textbf{Implementation} Given this task being based on 1d StopWords, the mapper and reducer are the same for both tasks. Our pig loaded the output from the MapReduce job, ordered it by count, limited it to 10 elements and stored it into a text file: 1ePig\_Output.txt \\ \\
\textbf{Notes/Reflection} We chose for this pig script to load its dataset from an hdfs cluster as opposed to a local storage location
\lstinputlisting[firstline=1,lastline=10,title=1ePig\_output]{output1e.txt}

\subsection{Tags}
\textbf{Assumption}: Write a MapReduce job to create a dictionary over unique tags in  \textit{Posts.xml}. The result should print the unique tags.\\ \\
\textbf{Implementation} This MapReduce job functions similarly to 1a WordCount, except it has to be aware of whether the post has a tag at all or not, being implemented when tags are extracted from the row \\ \\
\textbf{Notes/Reflection} The tags are separated by chevrons, and would be fused by cleanBody, so we elected to replace them with spaces. Some of the posts lacked any tag attribute, so we elected to pass over those.
\lstinputlisting[firstline=500,lastline=510,title=1f\_output]{output1f.txt}


% --TASK2--
\section{Task 2 Discover}
%This part of the task is about to looking through several 

\subsection{Counting}
\textbf{Assumption}: We chose to write a MapReduce job to count the total unique users there are in \textit{Users.xml}. The result will print out how many unique users there are.\\ \\
\textbf{Implementation} This MapReduce job functions similarly to 1c MoreThan10, except it counts all unique users instead of titles with more than 10 words. \\ \\
\textbf{Notes/Reflection} We interpreted "unique user" as a user with a unique Id attribute.
\lstinputlisting[firstline=1,lastline=1,title=2a\_output]{output2a.txt}

\subsection{Unique users}
\textbf{Assumption}: Write a MapReduce job based on 2a Counting to create a mapper  and a reducer functions in \textit{Users.xml}. The result should contain unique users in the dataset.  \\ \\
\textbf{Implementation} This MapReduce job functions as 2a Counting does, except it outputs all unique users instead of a count of them \\ \\
\textbf{Notes/Reflection} Here the implementation of cleanBody is modified with a join function as cleanBody returns a list where we want a string. Since usernames can contain spaces, we elected to instead use | as a separator in hadoop.
 \lstinputlisting[firstline=500,lastline=510,title=2b\_output]{output2b.txt}

\subsection{Top miners}
\textbf{Assumption}:Write a MapReduce job to find top 10 users based on attribute Reputation="x" in \textit{Users.xml}. The result will print out top 10 users based on their reputation. \\ \\
\textbf{Implementation} This mapper job is based on 2b Unique users, except instead of putting out name and id, it puts out name and the reputation attribute. Here, reputation is given to mapper\_core as a one-element list (mapper\_core called with arguments (name, [reputation])) due to it looping through its inputs. The reducer is the same as the reducer of 2b Unique users. The pig script functions the same way as in 1e Pig Top 10\\ \\
\textbf{Notes/Reflection} We elected here to use pig to order the MapReduce output in order to get experience.
\lstinputlisting[firstline=1,lastline=10,title=2c\_output]{output2c.txt}

\subsection{Top questions}
\textbf{Assumption}: Write a MapReduce job to find top 10 title questions PostTypeID="1"  based on attribute Score="x" in \textit{Posts.xml}.The result lists top 10 questions using id, question and the score.\\ \\
\textbf{Implementation} This mapper job functions similarly to 2c Top miners except it puts out 3 parsed attributes using the "triple"-mode of mapper\_core. The reducer performs the reduction and ordering, finding the 10 highest score values. It does this by inserting every input into a list instead of printing it, using pythons standard function string.sort to order it. It then iterates through the 10 highest values to give an output \\ \\
\textbf{Notes/Reflection} The reducer could potentially be improved by only keeping 10 elements in the list at once, changing them out as higher scores were encountered.
\lstinputlisting[firstline=1,lastline=10,title=2d\_output]{output2d.txt}

\subsection{Favorite questions }
\textbf{Assumption}: Write a MapReduce job to find top 10 title questions PostTypeID="1"  based on attribute FavoriteCount="x" in \textit{Posts.xml}.The result lists top 10 questions like id, question and the score.\\ \\ \\ \\
\textbf{Implementation} This MapReduce job is the same as 2d Top questions, except it extracts the FavoriteCount-attribute instead of the score \\ \\
\textbf{Notes/Reflection} Here, like 1f Tags, some posts had to be skipped due to their lacking any FavoriteCount-attribute.
\lstinputlisting[firstline=1,lastline=10,title=2e\_output]{output2e.txt}

\subsection{Average answers}
\textbf{Assumption}: \\ \\
\textbf{Implementation} This mapper functions similarly to  1c MoreThan10, except for having two counters, one for score and the other for users parsed, which are averaged at the end. The reducer is the same as the reducer in 2a Counting \\ \\
\textbf{Notes/Reflection} We have elected to not limit the amount of decimals.
\lstinputlisting[firstline=1,lastline=10,title=2f\_output]{output2f.txt}

\subsection{Countries}
\textbf{Assumption}: We chose to write a MapReduce job to discover users by countries in \textit{Users.xml}. The result lists different countries and corresponding users. \\ \\
\textbf{Implementation} This mapper functions similarly to 1f tags, except instead of cleaning the tags, it uses "|" as a separator, since spaces are part of location. \\ \\
\textbf{Notes/Reflection} We have elected to separate by location and not country, since by our judgement these locations are typed in by users. Places like "California, USA", "California, United States", and "Washington DC, USA" will thus be counted as different locations.
\lstinputlisting[firstline=1,lastline=10,title=2g\_output]{output2g.txt}

\subsection{Names}
\textbf{Assumption}We chose to write a MapReduce job to find the most popular names in \textit{Users.xml}. The result lists top 10 common names and how many.  \\ \\
\textbf{Implementation} This mapper functions similarly to 2g Countries, except it extracts the DisplayName-attribute in place of the Location-attribute. The reducer functions similarly to 2d Top questions, except it parses differently and has different text formatting for the output due to the decrease from 3 to 2 arguments \\ \\
\textbf{Notes/Reflection} Here the most common name is User. This is due to our choice of removing numbers in cleanBody, combining User1, User2, User3 etc.
\lstinputlisting[firstline=1,lastline=21,title=2h\_output]{output2h.txt}


\subsection{Answers }
\textbf{Assumption}: Write a simple python code to find how many titles of questions PostTypeId="x" have at least one answers based on attribute AnswerCount in \textit{Users.xml}. The result prints out how many questions have been answered.  \\ \\
\textbf{Implementation} This mapper functions in the same way as 1c MoreThan10 except it checks whether AnswerCount is larger than or equal to 1 instead of whether the title word length is larger than 10
\lstinputlisting[firstline=1,lastline=10,title=2i\_output]{output2i.txt}

% --TASK3--
\section{Task 3 Numbers}

\subsection{Bigram }
\textbf{Assumption}: We chose to write a MapReduce job to find the most common pair of adjacent words in \textit{Posts.xml}.For instance, "big data" or "Fast car" are examples of bigram. The result print the most common bigram   \\\\
\textbf{Implementation} This MapReduce is exactly the same as 1a WordCount, except we output each bigram in the body instead of each word. Because of this we he had to forego the mapper\_core for a print function inside a loop looping through every bigram. The reducer finds the most common bigram by iteratively comparing each elements second element (the bigrams count) against a variable declared maximum. \\ \\
\textbf{Notes/Reflection} In the print-loop we had to limit the loop to i in range(len(words)-1), since we printed both words[i] and words[i+1], and doing otherwise would result in and out of range error.
\lstinputlisting[firstline=1,lastline=10,title=3a\_output]{output3a.txt}

\subsection{Trigram}
\textbf{Assumption}: This task is based on 3a, to find three words that appear consecutively in \textit{Posts.xml}. The result print the most common trigram.  \\ \\
\textbf{Implementation} This is exactly the same as 3a Bigram, except we output trigrams instead of bigrams \\ \\
\textbf{Notes/Reflection} The mapper, as in 3a Bigram, limited the print-loop to range(len(words)-2) and printed words[i], words[i+1], and words[i+2].
\lstinputlisting[firstline=1,lastline=10,title=3b\_output]{output3b.txt}

\subsection{Combiner}
\textbf{Assumption}: Write a MapReduce job and add a combiner before  the data sent to the reducer. The result should be the same as a reducer, but the reducer recive a smaller volume of data.  \\ \\
\textbf{Implementation} The MapReduce function is the same as in 1a WordCount, with a combiner ahead of the print function in the mapper \\ \\
\textbf{Notes/Reflection} Having a combiner in a MapReduce job saves bandwidth and computational strain by decreasing the volume of data sent from the mapper. This is less relevant in our case, working with relatively small datasets.
\lstinputlisting[firstline=1,lastline=10,title=3c\_output]{output1a.txt}

\subsection{Useless}
\textbf{Assumption}:  We chose write a MapReduce job  to find how many times the word "useless" in \textit{Posts.xml} occurs in the body of questions PostTypeId="1". The result print how many times the word useless occurs.  \\ \\
\textbf{Implementation} This mapper functions similarly to 1c MoreThan10 and 2i Answers, except instead of checking for titles with more words than 10 or whether AnswerCount is larger than or equal to 1, it counts whether the word "useless" occurs in the body. \\ \\
\textbf{Notes/Reflection}
\lstinputlisting[firstline=1,lastline=10,title=3d\_output]{output3d.txt}
% --TASK4--
\section{Task 4 Search engine}
\subsection{Title index}
\textbf{Assumption}: We chose to write MapReduce job to create index over titles, bodies and answers of questions in \textit{Posts.xml}. We are after having a simple index that lists publications in which a search term/s and occur/s. The result is a list of words and their index appearance in posts LOL \\ \\
\textbf{Implementation} This mapper function parses through all rows in posts.xml. It then forks over whether PostTypeId is 1 or 2, extracting the relevant id, body and title from the row. It contains a combiner to speed up the process, removing duplicates. The reducer parses the input and  \\ \\
\textbf{Notes/Reflection}
%1792    1800
\lstinputlisting[firstline=1792,lastline=1800, title=4a\_output]{output4a.txt}



\section*{Conclusion}
MapReduce in Hadoop map a large dataset, then perform a reduce, on the specific output from the mapper. 

The idea behind MapReduce is that Hadoop can first map a large data set, and then perform a reduction on that content for specific results. A reduce function can be thought of as a kind of filter for raw data. The HDFS system then acts to distribute data across a network or migrate it as necessary.



% Do not modify this last lines
\end{document}